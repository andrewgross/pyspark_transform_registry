#!/usr/bin/env python3
"""
Complete example showing schema validation, MLflow storage, and runtime validation.
"""

import json

from pyspark_transform_registry.schema_constraints import (
    PartialSchemaConstraint,
    ColumnRequirement,
)
from pyspark_transform_registry.runtime_validation import RuntimeValidator


def main():
    print("üîç COMPLETE SCHEMA VALIDATION & MLFLOW STORAGE EXAMPLE")
    print("=" * 60)

    print("\n1Ô∏è‚É£ CREATING A SCHEMA CONSTRAINT")
    print("-" * 40)

    # Create a realistic schema constraint manually
    constraint = PartialSchemaConstraint(
        required_columns=[
            ColumnRequirement("customer_id", "integer"),
            ColumnRequirement("amount", "double"),
            ColumnRequirement("status", "string"),
        ],
        added_columns=[],  # Use empty for now to avoid serialization issues
        preserves_other_columns=False,  # Function only selects specific columns
        confidence=0.9,  # High confidence
        analysis_method="static_analysis_enhanced",
    )

    print("‚úÖ Schema constraint created!")
    print(f"   Confidence: {constraint.confidence}")
    print(f"   Analysis Method: {constraint.analysis_method}")
    print(f"   Preserves Other Columns: {constraint.preserves_other_columns}")

    print(f"\nüìã REQUIRED COLUMNS ({len(constraint.required_columns)}):")
    for col_req in constraint.required_columns:
        print(f"   - {col_req.name}: {col_req.type}")

    print(f"\nüìù ADDED COLUMNS ({len(constraint.added_columns)}):")
    for col_add in constraint.added_columns:
        print(f"   - {col_add.name}: {col_add.type}")

    print("\n2Ô∏è‚É£ MLFLOW STORAGE FORMAT")
    print("-" * 40)

    # Show how it's serialized for MLflow
    constraint_json = constraint.to_json()
    print("üì¶ JSON stored in MLflow tag 'schema_constraint':")

    # Pretty print for readability
    constraint_dict = json.loads(constraint_json)
    print(json.dumps(constraint_dict, indent=2))

    print("\nüè∑Ô∏è  Additional MLflow tags:")
    print(f"   schema_confidence: {constraint.confidence}")
    print(f"   schema_analysis_method: {constraint.analysis_method}")
    print(f"   schema_required_columns: {len(constraint.required_columns)}")
    print(f"   schema_added_columns: {len(constraint.added_columns)}")
    print(f"   schema_preserves_others: {constraint.preserves_other_columns}")

    print("\n3Ô∏è‚É£ LOADING FROM MLFLOW")
    print("-" * 40)

    # Simulate loading from MLflow
    print("üîÑ Simulating load from MLflow...")
    loaded_constraint = PartialSchemaConstraint.from_json(constraint_json)

    print("‚úÖ Constraint loaded successfully!")
    print(
        f"   Confidence matches: {constraint.confidence == loaded_constraint.confidence}",
    )
    print(f"   Required columns: {len(loaded_constraint.required_columns)}")
    print(f"   Added columns: {len(loaded_constraint.added_columns)}")

    print("\n4Ô∏è‚É£ RUNTIME VALIDATION")
    print("-" * 40)

    # Create a runtime validator (for demonstration only)
    _ = RuntimeValidator(strict_mode=False)

    print("üîç Testing validation with different DataFrames...")

    # We can't create actual Spark DataFrames without a SparkSession,
    # but we can show the validation logic with mock schemas

    print("\n‚úÖ VALID CASE:")
    print(
        "   Input DataFrame schema: [customer_id: int, amount: double, status: string, extra: string]",
    )
    print("   Required columns present: ‚úì customer_id, ‚úì amount, ‚úì status")
    print("   Validation result: PASS")
    print("   Extra columns: Ignored (preserves_other_columns=False)")

    print("\n‚ùå INVALID CASE 1 - Missing Required Column:")
    print("   Input DataFrame schema: [customer_id: int, amount: double]")
    print("   Missing required column: ‚úó status")
    print("   Validation result: FAIL")
    print("   Error: 'Input validation failed: Missing required column: status'")

    print("\n‚ö†Ô∏è  INVALID CASE 2 - Wrong Type:")
    print(
        "   Input DataFrame schema: [customer_id: int, amount: string, status: string]",
    )
    print("   Type mismatch: ‚úó amount (expected: double, got: string)")
    print("   Validation result: WARNING (in permissive mode) / FAIL (in strict mode)")
    print(
        "   Error: 'Input validation failed: Column amount type mismatch: expected double, got string'",
    )

    print("\n5Ô∏è‚É£ INTEGRATION WITH TRANSFORM FUNCTIONS")
    print("-" * 40)

    print("üîó How this integrates with PySpark Transform Registry:")
    print()
    print("1. REGISTRATION:")
    print("   register_function(func, name='my.transform', infer_schema=True)")
    print("   ‚Üí Static analysis creates PartialSchemaConstraint")
    print("   ‚Üí Constraint serialized to JSON and stored in MLflow tags")
    print("   ‚Üí Function and metadata saved to MLflow model registry")
    print()
    print("2. LOADING:")
    print("   load_function('my.transform', validate_input=True)")
    print("   ‚Üí MLflow model loaded")
    print("   ‚Üí Schema constraint JSON retrieved from run tags")
    print("   ‚Üí PartialSchemaConstraint deserialized")
    print("   ‚Üí Wrapper function created with validation")
    print()
    print("3. EXECUTION:")
    print("   transformed_df = loaded_func(input_df)")
    print("   ‚Üí RuntimeValidator checks input_df against constraint")
    print("   ‚Üí If validation passes: original function executed")
    print("   ‚Üí If validation fails: ValueError raised with details")
    print()
    print("4. MULTI-PARAMETER FUNCTIONS:")
    print(
        "   result = loaded_func(df, params={'threshold': 100, 'category': 'premium'})",
    )
    print("   ‚Üí Same validation process for DataFrame")
    print("   ‚Üí Parameters passed through to original function")

    print("\n6Ô∏è‚É£ VALIDATION MODES")
    print("-" * 40)

    print("üéõÔ∏è  Different validation modes available:")
    print()
    print("STRICT MODE (strict_validation=True):")
    print("   ‚Ä¢ Type mismatches ‚Üí ERROR")
    print("   ‚Ä¢ Missing columns ‚Üí ERROR")
    print("   ‚Ä¢ Warnings ‚Üí ERROR")
    print("   ‚Ä¢ Best for: Production systems requiring exact schema matches")
    print()
    print("PERMISSIVE MODE (strict_validation=False, default):")
    print("   ‚Ä¢ Type mismatches ‚Üí WARNING (may proceed)")
    print("   ‚Ä¢ Missing columns ‚Üí ERROR")
    print("   ‚Ä¢ Warnings ‚Üí LOG (execution continues)")
    print("   ‚Ä¢ Best for: Development and flexible production systems")
    print()
    print("NO VALIDATION (validate_input=False):")
    print("   ‚Ä¢ No schema checking")
    print("   ‚Ä¢ Direct function execution")
    print("   ‚Ä¢ Best for: Testing, debugging, or when you trust input")

    print("\nüéâ COMPLETE EXAMPLE FINISHED!")
    print("=" * 60)

    print("\nüí° KEY BENEFITS:")
    print("   ‚úÖ Deterministic validation before execution")
    print("   ‚úÖ Machine-to-machine operation without human intervention")
    print("   ‚úÖ Detailed error messages for debugging")
    print("   ‚úÖ Version compatibility across system upgrades")
    print("   ‚úÖ Confidence levels for analysis quality assessment")
    print("   ‚úÖ Flexible validation modes for different use cases")


if __name__ == "__main__":
    main()
